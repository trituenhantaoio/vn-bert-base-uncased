{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to use vn-bert-base-uncased - trituenhantao.io",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPrxvZxcfDro40jPuSzKsnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trituenhantaoio/vn-bert-base-uncased/blob/main/How_to_use_vn_bert_base_uncased_trituenhantao_io.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0lbTKuU_oMT"
      },
      "source": [
        "#Download and extract model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXzpw6zb_ScX",
        "outputId": "326dacd4-c5ce-47f0-c438-83d76fd5802e"
      },
      "source": [
        "!wget https://github.com/trituenhantaoio/vn-bert-base-uncased/releases/download/v0.1/vn-bert-base-uncased.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-20 14:23:54--  https://github.com/trituenhantaoio/vn-bert-base-uncased/releases/download/v0.1/vn-bert-base-uncased.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/314566681/a2b03100-2b86-11eb-8053-5434ba947a74?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201120%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201120T142308Z&X-Amz-Expires=300&X-Amz-Signature=af3c5151bc6dcc9e15e0a1e3ddbbe55500a0bdf7edb57753abe91a28e878c4d3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=314566681&response-content-disposition=attachment%3B%20filename%3Dvn-bert-base-uncased.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-11-20 14:23:54--  https://github-production-release-asset-2e65be.s3.amazonaws.com/314566681/a2b03100-2b86-11eb-8053-5434ba947a74?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201120%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201120T142308Z&X-Amz-Expires=300&X-Amz-Signature=af3c5151bc6dcc9e15e0a1e3ddbbe55500a0bdf7edb57753abe91a28e878c4d3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=314566681&response-content-disposition=attachment%3B%20filename%3Dvn-bert-base-uncased.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.171.35\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.171.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 413115006 (394M) [application/octet-stream]\n",
            "Saving to: ‘vn-bert-base-uncased.zip’\n",
            "\n",
            "vn-bert-base-uncase 100%[===================>] 393.98M  48.8MB/s    in 6.7s    \n",
            "\n",
            "2020-11-20 14:24:01 (58.7 MB/s) - ‘vn-bert-base-uncased.zip’ saved [413115006/413115006]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqIFuwyR_fEt",
        "outputId": "c25621c1-80a2-4110-b88c-a425435fcb72"
      },
      "source": [
        "!unzip vn-bert-base-uncased.zip -d vn-bert-base-uncased"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  vn-bert-base-uncased.zip\n",
            "  inflating: vn-bert-base-uncased/config.json  \n",
            "  inflating: vn-bert-base-uncased/pytorch_model.bin  \n",
            "  inflating: vn-bert-base-uncased/vocab.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OShycSf__tg9"
      },
      "source": [
        "#Install transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WOqwzLF_nMU"
      },
      "source": [
        "!pip install transformers -q"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoNVazFg_3FT"
      },
      "source": [
        "#Load and use tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZoM_BH6_-Ny",
        "outputId": "62f48df4-044d-4762-cb80-bcddbe304980"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"vn-bert-base-uncased\")\n",
        "\n",
        "# Tokenize input\n",
        "text = \"[CLS] Xin chào ! [SEP] Bạn đã vào Sài Gòn bao giờ chưa ? [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "\n",
        "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "masked_index = 9\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "\n",
        "print(tokenized_text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'xin', 'chao', '[UNK]', '[SEP]', 'ban', 'đa', 'vao', 'sai', '[MASK]', 'bao', 'gio', 'chua', '[UNK]', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RgO6zoaATDT"
      },
      "source": [
        "# Convert token to vocabulary indices\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "segments_ids = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XUYDZS_41S"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thvVUR-MAeO6"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained(\"vn-bert-base-uncased\")\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "# This is IMPORTANT to have reproducible results during evaluation!\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "segments_tensors = segments_tensors.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "    # See the models docstrings for the detail of the inputs\n",
        "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    # Transformers models always output tuples.\n",
        "    # See the models docstrings for the detail of all the outputs\n",
        "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
        "    encoded_layers = outputs[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1zLAeZPBZvI"
      },
      "source": [
        "# Test with MLM Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mKwuwALBcWx",
        "outputId": "380c9f34-2119-438f-b830-4a5c681ed45c"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertForMaskedLM.from_pretrained(\"vn-bert-base-uncased\")\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "segments_tensors = segments_tensors.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# confirm we were able to predict 'henson'\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at vn-bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqFL2GvDC2g4",
        "outputId": "2fbbec9f-e938-4227-e359-0d5fc2b05964"
      },
      "source": [
        "predicted_token == \"gon\" #sai gon"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}